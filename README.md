# Papers

TELECONFERENCE LINK FOUND [HERE!](https://umontana.zoom.us/j/9165828078)

## Week 0 (1/13 - 1/17)
LeCun, Bengio, and Hinton, Deep Learning, Nature, 2015

00\_deep\_learning.pdf

## Week 1 (1/20 - 1/24)
Baydin, Pearlmutter, Radul, and Siskind, Automatic Differentiation in Machine Learning, arXiv:1502.05767v4

Paper leads: Colligan and Kelly 

01\_automatic\_differentiation.pdf

## Week 2 (1/27 - 1/31)
Kingma and Ba, ADAM: A Method for Stochastic Optimization, ICLR 2015

Paper leads: Martin and St. George

02\_adam.pdf

## Week 3 (2/3 - 2/7)
Ben-Nun and Hoefler, Demistifying Parallel and Distributed Deep Learning, arXiv:1802.09941v2

Paper leads: Walling, Lucke, and Anderson

03\_parallelism.pdf

## Week 4 (2/10 - 2/14)
He, Zhang, Ren, and Sun, Deep Residual Learning for Image Recognition, arXiv:1512.03385v1

Paper leads: Dunbar, Stansberry, and Thibeau
04\_resnet\_cnn.pdf

## Week 5 (2/17 - 2/21)
Hochreiter and Schmidhuber, Long Short-Term Memory, Neural Computation, 1997

Paper leads: Mayer and Donahue
05\_lstm.pdf

## Week 6 (2/24 - 2/28)
Mnih et al., Human-level control through deep reinforcement learning, Nature, 2015

Paper leads: Brust, Finley, and Olson

06\_deepreinforcementlearning.pdf

## Week 7 (3/2 - 3/6)
Radford, Metz, and Chintala, Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, arXiv: 1511.06434v2

Paper leads: Bunt and Roddy

07\_dcgan.pdf

## Week 8 (3/9 - 3/13)
Doersch, Tutorial on Variational Autoencoders, arXiv: 1606:05908v2
Paper leads: Colligan, Meyer, and Finley

08\_variational\_autoencoders.pdf

## Week 9 (3/23 - 3/27)
Han, Mao, and Dally, Deep Compression: Compressing deep neural networks with pruning, trained quantization, and Huffman Coding. arXiv: 1510.00149v5

Paper leads: Lucke and Brinkerhoff
Quiz Link: https://forms.gle/xztyEsRQGgVnCNnR8

## Week 10 (3/30 - 4/03)
Elhoushi et al., DeepShift: Towards Multiplication-Less Neural Networks

Paper leads: Roddy and Anderson

## Week 11 (4/06 - 4/10)
Chen et al., Neural Ordinary Differential Equations

Paper leads: Stansberry, Brust, and Dunbar

## Week 12 (4/13 - 4/17)
Silver et al., Mastering the game of Go without human knowledge

Paper leads: Martin and St. George

## Week 13 (4/20 - 4/24)
Vaswani, et al.: Attention Is All You Need
Kitaev, Kaiser, and Levskaya: Reformer: The Efficient Transformer

Paper leads: Olson, Walling, and Bunt

## Week 14 (4/27 - 5/01)
Szegedy et al.: Intriguing properties of neural networks

Paper leads: Thibeau, Kelly, and Donahue

## Other interesting papers that we won't get to 
Below are a list of papers that would be interesting and suitable for the second half of the semester, but I also encourage you to come up with your own!

Ioffre et al. - Batch Normalization: Increasing learning speed by learning a normalizing step between layers.

Zhu et al. - Unpaired image-to-image translation: How to transfer characteristics between images without having to explicitly pair them.

Han et al. - SqueezeNet: A concrete example of the above.

Srivastava et al. - Dropout: a simple way to keep neural networks from overfitting: Randomly zeroing nodes is a pretty clever idea.

Andrieu et al. - An introduction to MCMC for Machine Learning: A review paper of the methods behind rigorous statistical sampling, or how to generate probability distributions over model parameters, rather than maximum likelihood estimators.

Blei et al. - Variational Inference: a review for statisticians: Similar to the above paper, but for a powerful technique called variation inference.

Wu et al. - Bridging the Gap between Human and Machine Translation: An overview of how Google's neural machine translation system works.

Graves et al. - Neural Turing Machines: Taking advantage of the Turing-completeness of RNNs by hooking one to an addressable and learnable external memory.

Johnson et al. - Perceptual Losses for Real-Time Style Transfer and Super-Resolution: Teaching neural networks to learn to make other neural networks happy, with applications to some real Men In Black type stuff.



