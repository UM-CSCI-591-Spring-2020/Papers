# Papers

## Week 0 (1/13 - 1/17)
LeCun, Bengio, and Hinton, Deep Learning, Nature, 2015

00\_deep\_learning.pdf

## Week 1 (1/20 - 1/24)
Baydin, Pearlmutter, Radul, and Siskind, Automatic Differentiation in Machine Learning, arXiv:1502.05767v4

Paper leads: Colligan and Kelly 

01\_automatic\_differentiation.pdf

## Week 2 (1/27 - 1/31)
Kingma and Ba, ADAM: A Method for Stochastic Optimization, ICLR 2015

Paper leads: Johnson and Martin 

02\_adam.pdf

## Week 3 (2/3 - 2/7)
Ben-Nun and Hoefler, Demistifying Parallel and Distributed Deep Learning, arXiv:1802.09941v2

Paper leads: Walling and Lucke

03\_resnet\_cnn.pdf

## Week 4 (2/10 - 2/14)
He, Zhang, Ren, and Sun, Deep Residual Learning for Image Recognition, arXiv:1512.03385v1

Paper leads: Dunbar and Stansberry

04\_resnet\_cnn.pdf

## Week 5 (2/17 - 2/21)
Hochreiter and Schmidhuber, Long Short-Term Memory, Neural Computation, 1997

Paper leads: Mayer and Tabibian

05\_lstm.pdf

## Week 6 (2/24 - 2/28)
Mnih et al., Human-level control through deep reinforcement learning, Nature, 2015

Paper leads: Brust and Finley

06\_deepreinforcementlearning.pdf

## Week 7 (3/2 - 3/6)
Radford, Metz, and Chintala, Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, arXiv: 1511.06434v2

Paper leads: Bunt and Roddy

07\_dcgan.pdf

## Paper Suggestions for Weeks 8-14
Below are a list of papers that would be interesting and suitable for the second half of the semester, but I also encourage you to come up with your own!

Szegedy et al. - Intriguing Properties of Neural Networks: How to construct adversarial images that cause a network to fail in surprising ways.

Viswani et al. - Attention is all you need: Introduction to the Transformer, the current state of the art architecture for language modelling.

Ioffre et al. - Batch Normalization: Increasing learning speed by learning a normalizing step between layers.

Zhu et al. - Unpaired image-to-image translation: How to transfer characteristics between images without having to explicitly pair them.

Han et al. - Deep Compression: A handful of techniques for reducing the size of neural networks without performance loss.

Han et al. - SqueezeNet: A concrete example of the above.

Srivastava et al. - Dropout: a simple way to keep neural networks from overfitting: Randomly zeroing nodes is a pretty clever idea.

Andrieu et al. - An introduction to MCMC for Machine Learning: A review paper of the methods behind rigorous statistical sampling, or how to generate probability distributions over model parameters, rather than maximum likelihood estimators.

Blei et al. - Variational Inference: a review for statisticians: Similar to the above paper, but for a powerful technique called variation inference.

Wu et al. - Bridging the Gap between Human and Machine Translation: An overview of how Google's neural machine translation system works.

Chen et al. - Neural Ordinary Differential Equations: An award winning paper that answers the question "what happens when you build a ResNet with an infinite number of layers?"

Graves et al. - Neural Turing Machines: Taking advantage of the Turing-completeness of RNNs by hooking one to an addressable and learnable external memory.

Johnson et al. - Perceptual Losses for Real-Time Style Transfer and Super-Resolution: Teaching neural networks to learn to make other neural networks happy, with applications to some real Men In Black type stuff.









